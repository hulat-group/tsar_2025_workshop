# üìä Results

This folder contains the outputs and evaluation metrics of our runs for the **TSAR 2025 Shared Task**.  
It is divided into two main components: **simplifications** (system outputs) and **metrics** (quantitative evaluation).

---

## üìù Simplifications
(ACTUALIZAR DE ACUERDO A LO NOMBRES...)
- `trial_simplifications.jsonl` ‚Üí system outputs for the **trial set**.  
- `test_simplifications.jsonl` ‚Üí system outputs for the **test set**.  
  
  ```json
  {
    "text_id": "number-a1/a2",
    "original": "...",
    "target_cefr": "A1/A2/B1",
    "reference": "...",
    "simplified": "..."
  }

- Additional runs included in [`../runs/`](../runs/).

Each file follows the shared task format:  

  ```json
  {
    "text_id": "number-a1/a2",
    "simplified": "...",
    "target_cefr": "A1/A2/B1"
  }

---

## üìê Metrics

We report evaluation results across several metrics:  

- **CEFR Compliance**: Weighted F1, Adjacent Accuracy, Exact match, RMSE  
- **Meaning Preservation**: MeaningBERT, BERTScore  
- **Similarity to References**: MeaningBERT, BERTScore  
- **AlignScore**  

### üìä Summary Table

| Metric                                 | LLaMA 3 8B (Reinforced)  | LLaMA 3 8B (Slightly Reinforced) | Ettin Decoder |
|----------------------------------------|--------------------------|----------------------------------|---------------|
| CEFR Compliance ‚Äì Weighted F1*         | 0.3000                   | 0.5200                           | 0.4800        |
| CEFR Compliance ‚Äì Adjacent Accuracy*   | 0.8500                   | 0.9750                           | 0.9300        |
| CEFR Compliance ‚Äì Exact                | 0.1750                   | 0.4750                           | ‚Äì             |
| CEFR Compliance ‚Äì RMSE*                | 1.1100                   | 0.7746                           | 0.8900        |
| Meaning Preservation ‚Äì MeaningBERT*    | 0.6532                   | 0.7170                           | 0.6901        |
| Meaning Preservation ‚Äì BERTScore*      | 0.8837                   | 0.8988                           | 0.9025        |
| Similarity to Refs ‚Äì MeaningBERT*      | 0.6384                   | 0.7075                           | 0.6243        |
| Similarity to Refs ‚Äì BERTScore*        | 0.8764                   | 0.8921                           | 0.8789        |
| AlignScore                             | 0.5600                   | 0.6038                           | 0.4300        |

---

## üìÇ Files Included (REVISAR)

- `trial_metrics.xlsx` ‚Üí results on the trial dataset.  
- `test_metrics.xlsx` ‚Üí results on the test dataset.  
- `comparison.xlsx` ‚Üí consolidated comparison across models.  
- `*.csv` files with raw metric outputs.  

---

‚ú® These results demonstrate the effectiveness of prompt-based approaches with **LLaMA 3 8B** and provide a baseline comparison with the **Ettin Decoder**.

# ğŸ“Š Results

This folder contains the outputs and evaluation metrics of our runs for the **TSAR 2025 Shared Task**.  
It is divided into two main components: **simplifications** (system outputs) and **metrics** (quantitative evaluation).

---

## ğŸ“ Simplifications

- `trial_simplifications.jsonl` â†’ system outputs for the **trial set**.  
- `test_simplifications.jsonl` â†’ system outputs for the **test set**.  
- Additional runs included in [`../runs/`](../runs/).

Each file follows the shared task format (`id`, `original`, `simplified`).

---

## ğŸ“ Metrics

We report evaluation results across several metrics:  

- **CEFR Compliance**: Weighted F1, Adjacent Accuracy, Exact match, RMSE  
- **Meaning Preservation**: MeaningBERT, BERTScore  
- **Similarity to References**: MeaningBERT, BERTScore  
- **AlignScore**  

### ğŸ“Š Summary Table

| Metric                                | LLaMA 3 8B (Reinforced) | LLaMA 3 8B (Slightly Reinforced) | Ettin Decoder |
|---------------------------------------|--------------------------|----------------------------------|---------------|
| CEFR Compliance â€“ Weighted F1         | 0.3000                   | 0.5200                           | 0.4800        |
| CEFR Compliance â€“ Adjacent Accuracy   | 0.8500                   | 0.9750                           | 0.9300        |
| CEFR Compliance â€“ Exact               | 0.1750                   | 0.4750                           | â€“             |
| CEFR Compliance â€“ RMSE                | 1.1100                   | 0.7746                           | 0.8900        |
| Meaning Preservation â€“ MeaningBERT    | 0.6532                   | 0.7170                           | 0.6901        |
| Meaning Preservation â€“ BERTScore      | 0.8837                   | 0.8988                           | 0.9025        |
| Similarity to Refs â€“ MeaningBERT      | 0.6384                   | 0.7075                           | 0.6243        |
| Similarity to Refs â€“ BERTScore        | 0.8764                   | 0.8921                           | 0.8789        |
| AlignScore                            | 0.5600                   | 0.6038                           | 0.4300        |

---

### ğŸ“· Visualization

Below is the summary of metrics as a comparative chart:

![Metrics Table](./d7f9b1dd-c9f4-463b-b5ec-51606caed536.png)

---

## ğŸ“‚ Files Included

- `trial_metrics.xlsx` â†’ results on the trial dataset.  
- `test_metrics.xlsx` â†’ results on the test dataset.  
- `comparison.xlsx` â†’ consolidated comparison across models.  
- `*.csv` files with raw metric outputs.  

---

âœ¨ These results demonstrate the effectiveness of prompt-based approaches with **LLaMA 3 8B** and provide a baseline comparison with the **Ettin Decoder**.
